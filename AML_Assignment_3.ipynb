{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_Assignment_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Animesh-Guchhait/Q-learning-Reinforcement-/blob/main/AML_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary Libraries"
      ],
      "metadata": {
        "id": "9-E90s4hhsU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9-zJjXbKZsd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Racetrack Environment building"
      ],
      "metadata": {
        "id": "cFEMJqCdh2uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This class is used to represent the race track as the Racetrack environment\n",
        "\n",
        "class Racetrack:\n",
        "  def __init__(self, track_file, reward=10):  #Initializing the Racetrack by reading the race track file\n",
        "    with open(track_file,'r') as f:\n",
        "      track = f.readlines()                                     \n",
        "    self.arr = self.build_track(track)                       #Building the Racetrack\n",
        "    self.m, self.n = self.arr.shape[0], self.arr.shape[1]\n",
        "    self.reward = reward                                      #Initializing the reward for going out of track, collision or reaching goal\n",
        "   \n",
        "\n",
        "\n",
        "  def build_track(self, track):      #build track helps to build the racetrack as Racetrack environment\n",
        "    \n",
        "    # 0: Start State (S)\n",
        "    # 1: Other State (O)\n",
        "    # 2: Obstacles State(X)\n",
        "    # 3: Finish State(F) \n",
        "\n",
        "    if track[-1]=='\\n':\n",
        "        track.pop()\n",
        "      \n",
        "    m = len(track)\n",
        "      \n",
        "    arr = []\n",
        "\n",
        "    for i in range(m-1,-1,-1):\n",
        "      t = []\n",
        "      for j in range(len(track[i])):\n",
        "        if track[i][j]!='\\n':\n",
        "          if track[i][j] =='S':\n",
        "            t.append(0)\n",
        "          elif track[i][j] =='O':\n",
        "            t.append(1)\n",
        "          elif track[i][j] == 'X':\n",
        "            t.append(2)\n",
        "          elif track[i][j] == 'F':\n",
        "            t.append(3)\n",
        "      arr.append(t)\n",
        "\n",
        "    arr = np.array(arr)\n",
        "    return arr\n",
        "  \n",
        "  def plot_track(self):                 # to plot the race track\n",
        "    cmap = ListedColormap(['r','white','k','g'])\n",
        "    \n",
        "    t = np.zeros((self.m, self.n))\n",
        "    for i in range(self.m-1, -1, -1):\n",
        "      for j in range(self.n):\n",
        "        t[(self.m-1) - i][j] = self.arr[i][j]\n",
        "\n",
        "    plt.matshow(t, cmap = cmap)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  def plot_episode(self,episode, end_state):      # to plot an episode of the agent(car)\n",
        "\n",
        "    t1 = self.arr.copy()\n",
        "    for ep in episode:\n",
        "      s = ep[0]\n",
        "      i,j = s\n",
        "      t1[i,j] = 4\n",
        "\n",
        "    if (end_state[0]>=(self.m-1)) and (0<=end_state[1]<self.n):\n",
        "      t1[self.m-1,end_state[1]] = 4 \n",
        "    elif (0<=end_state[0]<self.m) and (0<=end_state[1]<self.n):\n",
        "      t1[end_state[0], end_state[1]] = 4\n",
        "    \n",
        "    cmap = ListedColormap(['r','white','k','g','pink'])\n",
        "    \n",
        "    t = np.zeros((self.m, self.n))\n",
        "    for i in range(self.m-1, -1, -1):\n",
        "      for j in range(self.n):\n",
        "        t[(self.m-1) - i][j] = t1[i][j]\n",
        "\n",
        "    plt.imshow(t, cmap = cmap)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "  def update_state(self, s, a):      #to get the new state of the agent based on it's past state and action chosen \n",
        "    s_new = (s[0] + a[0], s[1] + a[1])\n",
        "    return (s_new)\n",
        "\n",
        " \n",
        "  def isTerminal(self, s):             #to check if a given state is a terminal state\n",
        "    i = s[0]\n",
        "    j = s[1]\n",
        "    if (i<0) or (j<0) or (j>=self.n) or(i>=self.m):     #went out of racetrack\n",
        "      return True\n",
        "    if (self.arr[i,j]==2) or (self.arr[i,j]==3):        #hitting an obstacle or reaching goal\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def get_reward(self,state, hit_obstacle=False):        #to get reward for current state\n",
        "    if hit_obstacle ==True:\n",
        "      return -(self.reward)\n",
        "    i,j = state[0], state[1]\n",
        "    if (i<0) or (j<0) or (j>=self.n):\n",
        "      return -(self.reward)\n",
        "    elif (i>=self.m):\n",
        "      return self.reward\n",
        "    elif self.arr[i,j]==3:\n",
        "      return self.reward\n",
        "    elif self.arr[i,j]==2:\n",
        "      return -(self.reward)\n",
        "    return -1\n",
        "\n",
        "  def sample_start_state(self):          # to sample a start state for the agent\n",
        "    j = np.random.randint(0,self.n)\n",
        "    return (0,j)\n",
        "    "
      ],
      "metadata": {
        "id": "73y1ODdxKbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q Learning Method**\n",
        "\n",
        "In this assignment,We are useing the Q-Learning Algorithm.Here the agent(car) is represented by the TD_Agent class. "
      ],
      "metadata": {
        "id": "EtP_lV-eigYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Class to represent the agent(car)\n",
        "class TD_Agent:\n",
        "  def __init__(self, epsilon = 0.2):\n",
        "    #Building the agent with parameters epsilon, the action space possible and initial velocity\n",
        "    self.epsilon = epsilon\n",
        "    self.actions = self.actions()\n",
        "    self.velocity = (0,0)\n",
        "\n",
        "\n",
        "  def initialize(self, states, env):\n",
        "    #Initialize  q(s,a) arbitrarily for each state and action except the target state which has q(s,a) = 0\n",
        "    self.q = {}\n",
        "    for s in states:\n",
        "      for a in self.actions:\n",
        "        s_a = (s,a)\n",
        "        if (not env.isTerminal(s)):\n",
        "          self.q[s_a] = np.random.normal(-1,1)\n",
        "        else:\n",
        "          self.q[s_a] = 0                 \n",
        "        \n",
        "  def get_action(self,state, learn = True):  # to get the possible actions for a given state  \n",
        "    if learn==True:\n",
        "      t = -1\n",
        "      prob_sample = np.random.uniform(0,1)\n",
        "      if prob_sample <= self.epsilon:     #to explore\n",
        "        while True:\n",
        "          action = random.choice(self.actions)\n",
        "          if self.action_valid(action):\n",
        "            a = action\n",
        "            break\n",
        "      \n",
        "      else:           #to exploit\n",
        "        t = -999 \n",
        "        best_actions = []\n",
        "        for action in self.actions:\n",
        "          x = (state, action)\n",
        "          \n",
        "          if (self.q[x]>t) and (self.action_valid(action)):\n",
        "            best_actions = []\n",
        "            t = self.q[x]\n",
        "            best_actions.append(action)\n",
        "          elif (self.q[x]==t) and (self.action_valid(action)):\n",
        "            t = self.q[x]\n",
        "            best_actions.append(action)\n",
        "\n",
        "        if len(best_actions)>1:\n",
        "          a = random.choice(best_actions)\n",
        "        else:\n",
        "          a = best_actions[0]\n",
        "    \n",
        "    else:       #sample action based on learnt agent\n",
        "      t = -999 \n",
        "      best_actions = []\n",
        "      for action in self.actions:\n",
        "        x = (state, action)\n",
        "        \n",
        "        if (self.q[x]>t) and (self.action_valid(action)):\n",
        "          best_actions = []\n",
        "          t = self.q[x]\n",
        "          best_actions.append(action)\n",
        "        elif (self.q[x]==t) and (self.action_valid(action)):\n",
        "          best_actions.append(action)\n",
        "\n",
        "      if len(best_actions)>1:\n",
        "        a = random.choice(best_actions)\n",
        "      else:\n",
        "        a = best_actions[0]\n",
        "    return a\n",
        "\n",
        " \n",
        "  def action_valid(self, a):   #to check if action chosen is a valid action with respect to agent constraints\n",
        "    vx = self.velocity[0] + a[0]\n",
        "    vy = self.velocity[1] + a[1]\n",
        "\n",
        "    if (vx==0 and vy==0):   #velocity cannot be zero except at start position\n",
        "      return False\n",
        "\n",
        "    if (np.abs(vx)<5) and (np.abs(vy)<5):       #absolute value of each velocity component must be less than 5\n",
        "      return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "  def update_velocity(self, a):  # to update velocity of agent(car)\n",
        "    vx = self.velocity[0] + a[0]\n",
        "    vy = self.velocity[1] + a[1]\n",
        "    self.velocity = (vx, vy)\n",
        "\n",
        "  def reset_car(self):  # to reset car velocity after end of episode\n",
        "    self.velocity = (0,0)\n",
        "\n",
        "\n",
        "  def get_a_star(self, s):    # to chose greedily the action which gives max(q(s,a)) for a given state  \n",
        "    t = -999\n",
        "    best_actions = []\n",
        "    for a in self.actions:\n",
        "      state = (s,a)\n",
        "      if self.q[state] > t:\n",
        "        best_actions = []\n",
        "        t = self.q[state]\n",
        "        best_actions.append(a)\n",
        "      elif self.q[state]==t:\n",
        "        best_actions.append(a)\n",
        "    \n",
        "    if len(best_actions)>1:\n",
        "      a_star = random.choice(best_actions)\n",
        "    else:\n",
        "      a_star = best_actions[0]\n",
        "\n",
        "    return a_star\n",
        "\n",
        "  def actions(self):   # to build possible actions of the agent based on the problem statement\n",
        "    possible_actions = [-1, 0, 1]   # -1:decrease velocity by one, 0: keep velocity unchanged, 1: increase velocity by 1\n",
        "    return (list(itertools.product(possible_actions, possible_actions)))  \n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sgxxpsc3KncK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to determine if a move of the agent from state s to state s_new causes a collision. "
      ],
      "metadata": {
        "id": "nVlQw9Fhj9aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_hit_obstacle(env, s, s_new):\n",
        "  x_a, y_a = s\n",
        "  x_b, y_b = s_new\n",
        "\n",
        "  if x_a > x_b:\n",
        "    min_x, max_x = x_b, x_a\n",
        "  else:\n",
        "    min_x, max_x = x_a, x_b\n",
        "\n",
        "  if y_a > y_b: \n",
        "    min_y, max_y = y_b, y_a\n",
        "  else:\n",
        "    min_y, max_y = y_a, y_b\n",
        "\n",
        "  dx = (max_x - min_x) + 1\n",
        "  dy = (max_y - min_y) + 1\n",
        "  t = env.arr[min_x:(min_x + dx),min_y:(min_y+dy)]\n",
        "  \n",
        "  for elem in t.flatten():\n",
        "    if elem==2:\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "Qs92JKmzKuNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Q-learning algorithm"
      ],
      "metadata": {
        "id": "Xx_fPkY0kB23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_ep = None\n",
        "# implement the Q-Learning algorithm\n",
        "def Q_L(agent, env, alpha = 0.5,discount_factor = 0.9, epsilon = 0.2, episodes = 50000, goals_desired = 10000):\n",
        "  global last_ep\n",
        "  \n",
        "  #Initialize Q(S,A)\n",
        "  states = list(itertools.product(range(env.m), range(env.n)))\n",
        "  agent.initialize(states,env)\n",
        "\n",
        "  episode_number = 0\n",
        "  goals_crossed = 0\n",
        "  \n",
        "\n",
        "  for episode_number in tqdm(range(episodes)):\n",
        "\n",
        "    start_state = env.sample_start_state()\n",
        "    \n",
        "    s = start_state\n",
        "    a = agent.get_action(s)\n",
        "    agent.update_velocity(a)\n",
        "    r = env.get_reward(s)\n",
        "\n",
        "    episode = [(s,a,r)]\n",
        "    \n",
        "    s_new = env.update_state(s, agent.velocity)\n",
        "    hit_obstacle = move_hit_obstacle(env, s,s_new)\n",
        "\n",
        "    while (not env.isTerminal(s_new)) and (not hit_obstacle):\n",
        "      \n",
        "      a_star = agent.get_a_star(s_new)\n",
        "      agent.q[(s,a)] = agent.q[(s,a)] + alpha*(r + (discount_factor* agent.q[(s_new, a_star)]) - agent.q[(s,a)])\n",
        "\n",
        "\n",
        "      r = env.get_reward(s_new)\n",
        "      s = s_new\n",
        "      a = agent.get_action(s)\n",
        "      agent.update_velocity(a)\n",
        "\n",
        "      s_new = env.update_state(s, agent.velocity)\n",
        "      hit_obstacle = move_hit_obstacle(env,s,s_new)\n",
        "      episode.append((s,a,r))\n",
        "    \n",
        "    #Final Terminal state reward\n",
        "    \n",
        "    agent.q[(s,a)] = agent.q[(s,a)] + alpha*(r + (discount_factor * env.get_reward(s_new,hit_obstacle)) - agent.q[(s,a)])     \n",
        "\n",
        "    agent.reset_car()\n",
        "    \n",
        "    if hit_obstacle==False:\n",
        "      if s_new[0]>=env.m:        #acceleration caused car to go out of track crossing the finish line meanwhile\n",
        "        goals_crossed+=1\n",
        "        last_ep = (episode,s_new)\n",
        "      \n",
        "      if (0<=s_new[0]<env.m) and (0<=s_new[1]<env.n):\n",
        "        if env.arr[s_new[0], s_new[1]] == 3:\n",
        "          goals_crossed +=1\n",
        "          last_ep = (episode,s_new)\n",
        "\n",
        "    \n",
        "    if goals_crossed >=goals_desired:\n",
        "      print (\"Goals crossed surpassed goals desired to stop. Stopping Learning!\")\n",
        "      break\n",
        "    \n",
        "  return goals_crossed"
      ],
      "metadata": {
        "id": "gYOus50JK1K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show_trace is used to show states, actions and rewards of each time step."
      ],
      "metadata": {
        "id": "1TbQSUc8kfyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_trace(episode, end_state, end_reward):\n",
        "  trace = {'Time': [], 'State': [], 'Action': [], 'Reward':[]}\n",
        "  for i, ep in enumerate(episode):\n",
        "    s, a, r = ep\n",
        "    trace['Time'].append(i+1)\n",
        "    trace['State'].append(s)\n",
        "    trace['Action'].append(a)\n",
        "    trace['Reward'].append(r)\n",
        "\n",
        "  trace['Time'].append(len(episode)+1)\n",
        "  trace['State'].append(end_state)\n",
        "  trace['Action'].append(\"X\")\n",
        "  trace['Reward'].append(end_reward)\n",
        "\n",
        "  result = pd.DataFrame(trace)\n",
        "  print (result)"
      ],
      "metadata": {
        "id": "P-2oBBTaR9IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6YMhCO_LEWb",
        "outputId": "da5ee02e-bf1a-4d5a-dbaf-2e02cd0671da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Track 1**\n",
        "\n",
        "Building the racetrack environment for track 1 and applying the Q learning algorithm for the agent on this racetrack environment.\n",
        "\n",
        " Here we took a steeper reward of 30 and penalty 30 for track 1 as a reward of 10 and penalty of 10 was causing the algorithm to take a lot more time to converge to 10K episodes where agent crossed goal state."
      ],
      "metadata": {
        "id": "ri6Keja5kxVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Racetrack('/content/drive/MyDrive/track-1.txt', reward = 30)\n",
        "td_agent = TD_Agent()\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.2, discount_factor = 0.95, episodes = 5000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn0zHnaRK7V7",
        "outputId": "c0da16e4-6ab0-4d8a-f6da-1b46535bfc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 3038558/5000000 [24:00<15:29, 2109.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goals crossed surpassed goals desired to stop. Stopping Learning!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episode, end_state = last_ep           # Plot of the path of the agent car\n",
        "end_reward = env.get_reward(end_state)\n",
        "env.plot_episode(episode, end_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "8cct5RNJRxQA",
        "outputId": "149f3954-e3e6-4256-9597-b6af82f4b9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAADnCAYAAABWvGk6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADSElEQVR4nO3dUWrbQBRAUU/JkprvtjvLztJ+p3tSN1BJ0WUiO9E5v45AYC4D4fnNWJblBhzz7d4vAJ+RcCAQDgTCgUA4EDxtfThexuF/uS2/3g6/xHh9PvwMfLTlZRlrnzlxIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQjJ2fTvtdNVdmyBNmEg4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBBsbvI8zZ+/x/7+x/ePeQ94JycOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQPMas2gnGWN0tx0Q7Cy6/DCcOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQCAeCywx5XmX4kHM4cSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EIyd4cfHnIw8ekv17eamaorV9a9OHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBJ9zyPMshkmvzpAnzCQcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAg2LyufYzVGbdLOO2K96PDpCcNkvr+179/Jw4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwKbPK/CVtLCJk+YSTgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQCAcCs2rM9bVm4syqwUzCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAkOesM6QJ8wkHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIHi69wucZYzVeT0m2hkanuMBtoU6cSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4ENjkCets8oSZhAOBcCAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgSX2eTJxU3e/unEgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFAYJMnrLPJE2YSDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgcBCQq7BQkK4P+FAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIhAPB9pDnWN3HBvfz++34Mz+fjz+zsazTiQOBcCAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgWDv1mngP5w4EAgHAuFAIBwIhAOBcCD4B6eaU7bdRUUdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_trace(episode, end_state, end_reward) # states actions and rewards for each step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bYaSzBmSfln",
        "outputId": "f2c38b03-051e-44e0-c842-20a0312cab2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Time    State   Action  Reward\n",
            "0      1  (0, 13)   (1, 1)      -1\n",
            "1      2  (1, 14)  (1, -1)      -1\n",
            "2      3  (3, 14)  (0, -1)      -1\n",
            "3      4  (5, 13)  (0, -1)      -1\n",
            "4      5  (7, 11)  (-1, 0)      -1\n",
            "5      6   (8, 9)   (0, 0)      -1\n",
            "6      7   (9, 7)   (0, 0)      -1\n",
            "7      8  (10, 5)   (0, 0)      -1\n",
            "8      9  (11, 3)   (0, 1)      -1\n",
            "9     10  (12, 2)   (0, 1)      -1\n",
            "10    11  (13, 2)   (0, 0)      -1\n",
            "11    12  (14, 2)  (1, -1)      -1\n",
            "12    13  (16, 1)        X      30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Track 2**\n",
        "\n",
        "Building the racetrack environment for track 2 and applying the Q learning algorithm for the agent on this racetrack environment.\n",
        "\n",
        "Here we took a steeper reward of 50 and penalty 50 for track 2 as a reward of 30 and penalty of 30 was causing the algorithm to take a lot more time to converge to 10K episodes where agent crossed goal state."
      ],
      "metadata": {
        "id": "AnB65roAnDCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Racetrack('/content/drive/MyDrive/track-2.txt', reward = 50)\n",
        "td_agent = TD_Agent()\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.3, discount_factor = 0.95, episodes = 5000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZajJGYSly2",
        "outputId": "99e09729-ba0f-4e69-f339-1b0b3875e46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000000/5000000 [40:43<00:00, 2046.60it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5055"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though we have increases the value of reward to 50, we could not achive targeted the 10k episodes goal. We have got 5055. So we need more iteration (more than 5000000).But let see how it's perfromence with this much learning."
      ],
      "metadata": {
        "id": "BOY_AjeEw6wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode, end_state = last_ep           # Plot of the path of the agent car\n",
        "end_reward = env.get_reward(end_state)\n",
        "env.plot_episode(episode, end_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "8pyElmrInq4W",
        "outputId": "74e3f1b5-5cd3-4031-e8b5-02dbed1049d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAADnCAYAAACjQuKKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADQ0lEQVR4nO3dQU7cQBBAUXeUI4V1kptxs8Aa7uQs8jeJoMFkPDOM31uCJQzoqyVUVI91XRdgWb5c+gXgWogBIgaIGCBigHydfXLcD39q4r+tP582PT9+3e30Jn+s9+t46eNOBogYIGKAiAEiBogYIGKAiAEiBogYIGKAjDf+081sErfIbBLMiAEiBogYIGKAiAEiBogYIGKAiAEiBogYIGKAiAEiBogYIGKAiAEiBogYIGKAiAEiBsj0Tjc4icfnbc9//7bPe7zByQARA0QMEDFAxAARA0QMEDFAxAARA0QMkOmdbmMMd7rxlzfuAPws3OkGM2KAiAEiBogYIGKAiAEiBogYIGKAiAEiBsh0idiNDGXta+uCrK12Xqg1xosza4fkZICIASIGiBggYoCIASIGiBggYoCIASIGiAsO/7X3ZXx7zzLt7SPvf6ELC7dyMkDEABEDRAwQMUDEABEDRAwQMUDEABEDZHrB4bIs17U46YbnYi5l696kG9ml5YJDmBEDRAwQMUDEABEDRAwQMUDEABEDRAyQzzWbdEQ773HaPJv08LTp+WVZrnE+zGwSzIgBIgaIGCBigIgBIgaIGCBigIgBIgbIae902/s+tA846F6gdzva9zvjZICIASIGiBggYoCIASIGiBggYoCIASIGyHxv0uPztsGV69uPc7zZpCucD7vCd7I3CWbEABEDRAwQMUDEABEDRAwQMUDEABED5Ob3Jh3O3j/Trb/jT8TJABEDRAwQMUDEABEDRAwQMUDEABEDRAyQ+d6kZdl3idAZZpkOtzeJ97A3CWbEABEDRAwQMUDEABEDRAwQMUDEABED5LKzSWewdTaJ27euq9kkmBEDRAwQMUDEABEDRAwQMUDEABEDRAwQMUBOe8HhVmdYImYpGO/lZICIASIGiBggYoCIASIGiBggYoCIASIGyGlnk84wawR7cTJAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA0QMEDFAxAARA2S+N2mMfb/6w9O253/c7fMeHMsr9/w5GSBigIgBIgaIGCBigIgBIgaIGCBigIgBMtZX5jTgaJwMEDFAxAARA0QMEDFAfgP8xWxEJq7MQgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though learning have not achived the targeted episodes goal, the agent car reached it's destination smoothly for this racetrack."
      ],
      "metadata": {
        "id": "gJaoiu4EyFbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_trace(episode, end_state, end_reward) # states actions and rewards for each step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzzxQIVYnvFk",
        "outputId": "05e45920-6559-46ef-9005-11511a6cc5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Time     State    Action  Reward\n",
            "0      1    (0, 2)   (1, -1)      -1\n",
            "1      2    (1, 1)    (0, 1)      -1\n",
            "2      3    (2, 1)    (0, 1)      -1\n",
            "3      4    (3, 2)   (1, -1)      -1\n",
            "4      5    (5, 2)  (-1, -1)      -1\n",
            "5      6    (6, 1)   (-1, 0)      -1\n",
            "6      7    (6, 0)    (1, 1)      -1\n",
            "7      8    (7, 0)    (0, 1)      -1\n",
            "8      9    (8, 1)    (1, 1)      -1\n",
            "9     10   (10, 3)   (-1, 0)      -1\n",
            "10    11   (11, 5)   (0, -1)      -1\n",
            "11    12   (12, 6)   (-1, 0)      -1\n",
            "12    13   (12, 7)  (-1, -1)      -1\n",
            "13    14   (11, 7)   (-1, 1)      -1\n",
            "14    15    (9, 8)   (1, -1)      -1\n",
            "15    16    (8, 8)    (0, 1)      -1\n",
            "16    17    (7, 9)    (0, 1)      -1\n",
            "17    18   (6, 11)   (1, -1)      -1\n",
            "18    19   (6, 12)   (1, -1)      -1\n",
            "19    20   (7, 12)   (1, -1)      -1\n",
            "20    21   (9, 11)    (0, 1)      -1\n",
            "21    22  (11, 11)    (1, 1)      -1\n",
            "22    23  (14, 12)   (1, -1)      -1\n",
            "23    24  (18, 12)         X      50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Track 3**\n",
        "\n",
        "Building the racetrack environment for track 3 and applying the Q learning algorithm for the agent on this racetrack environment.\n",
        "\n",
        "Here we took a steeper reward of 40 and penalty 40 for track 3 "
      ],
      "metadata": {
        "id": "3L1Fhkwwn8B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Racetrack('/content/drive/MyDrive/track-3.txt', reward = 40)\n",
        "td_agent = TD_Agent()\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.3, discount_factor = 0.95, episodes = 5000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxvrJXfgnwJ0",
        "outputId": "1719222f-f655-435f-caee-9f93e2983288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 920491/5000000 [05:30<24:23, 2786.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Goals crossed surpassed goals desired to stop. Stopping Learning!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episode, end_state = last_ep           # Plot of the path of the agent car\n",
        "end_reward = env.get_reward(end_state)\n",
        "env.plot_episode(episode, end_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "ExGAfzvsn19N",
        "outputId": "329c6c49-cf89-4cc5-84bc-91097faeaf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAADnCAYAAABWvGk6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADUElEQVR4nO3dS47TQBRA0S7EkmCM2FnvDBg3e6reAHbkq8L5+JxpYsmTq5Kil1djzvkGHPPl3i8Az0g4EAgHAuFAIBwIvu59ON7H4Z/c5s+Pwy8xfn0//Az8b/N9jq3PnDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAjGjb9O+181V2bIE1YSDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EOxu8nxYf/4ef+bHt/XvwWU5cSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgcBCwsXG2Nxhx5OZ063TsJRwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKB4DkXEhYnLTG8MTTLi3DiQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgsMlzj9utr84mT1hJOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQHCdTZ5nKYOhRxkkvTsnDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAps8r8JW0sImT1hJOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQGDIk21nbCV9e3vkYVJDnrCScCAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgRunWZbmSE7a77tzpw4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIdoc8x9jcx8Yd3VgiuY6bqjc5cSAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4ELh1+hkZvjyLW6dhJeFAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIhAOBIc/VDGC+EkOesJJwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBYPe69sszsMkGJw4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIB4Ln3OT5YsOXY2wujOSO5pw2ecJKwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBA8xkLCo7NnDzx3VtyYF+QBOXEgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBDsD3metSjv98ex71vgxxl2hm+dOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCG7dOg38gxMHAuFAIBwIhAOBcCAQDgSf7S9bQHQW/b0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_trace(episode, end_state, end_reward) # states actions and rewards for each step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SrPMHpFn4C8",
        "outputId": "d39d4251-d252-46bb-cc5a-ae3bf1236efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Time    State    Action  Reward\n",
            "0      1   (0, 2)    (1, 0)      -1\n",
            "1      2   (1, 2)    (0, 1)      -1\n",
            "2      3   (2, 3)    (0, 1)      -1\n",
            "3      4   (3, 5)   (0, -1)      -1\n",
            "4      5   (4, 6)    (0, 1)      -1\n",
            "5      6   (5, 8)    (0, 0)      -1\n",
            "6      7  (6, 10)   (0, -1)      -1\n",
            "7      8  (7, 11)   (0, -1)      -1\n",
            "8      9  (8, 11)  (-1, -1)      -1\n",
            "9     10  (8, 10)    (1, 0)      -1\n",
            "10    11   (9, 9)   (0, -1)      -1\n",
            "11    12  (10, 7)   (-1, 1)      -1\n",
            "12    13  (10, 6)    (0, 0)      -1\n",
            "13    14  (10, 5)    (1, 1)      -1\n",
            "14    15  (11, 5)   (0, -1)      -1\n",
            "15    16  (12, 4)    (1, 0)      -1\n",
            "16    17  (14, 3)   (1, -1)      -1\n",
            "17    18  (17, 1)         X      40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the racetrack environment for track 3 and applying the Q learning algorithm for the agent on this racetrack environment.\n",
        "\n",
        "Here we took a steeper reward of 10 and penalty 10 for track 3"
      ],
      "metadata": {
        "id": "yq2LbgHt5W3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Racetrack('/content/drive/MyDrive/track-3.txt', reward = 10)\n",
        "td_agent = TD_Agent()\n",
        "Q_L(td_agent, env, alpha = 0.6, epsilon = 0.3, discount_factor = 0.95, episodes = 5000000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VAWub-r1v29",
        "outputId": "72a65411-fc28-4eab-b3ff-28f54d6cda08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000000/5000000 [04:17<00:00, 19393.16it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8074"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have used reward=10 and can not manage to get 10k episodes"
      ],
      "metadata": {
        "id": "54ydYK2k5ieG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode, end_state = last_ep           # Plot of the path of the agent car\n",
        "end_reward = env.get_reward(end_state)\n",
        "env.plot_episode(episode, end_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Z_F_2bp514kD",
        "outputId": "964d4733-b57d-4aff-8fa0-89f4d1c3f6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAADnCAYAAABWvGk6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADOklEQVR4nO3dQY7TQBBA0TSaIzFr4GZzM2A93MlcAHeUr8Zxkve2kSVvvkoa1ZTHtm0X4DZf7v0C8IiEA4FwIBAOBMKB4G324/gYp/yT2/bj8+Znxs/3//AmPLPtYxt7v5k4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIxpV/nT7lkiccxJInrCQcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIB4LpV6efyu8/tz/z7evNj4yxe8OOBzM71mniQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCIQDgXAg8NVp2Oer07CScCAQDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgeB1LnkWB13/5PGYOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBAIBwLhQCAcCFzyXO2IxVDLp0dxyRNWEg4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoFAOBBY8nwVFkMLS56wknAgEA4EwoFAOBAIBwLhQCAcCIQDgXAgEA4EwoHAkifss+QJKwkHAuFAIBwIhAOBcCAQDgTCgUA4EAgHAuFA8HbvF+AgDhIuZeJAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIhAOBcCCYHiQcYzhIeEJXjkiyjoOEsJJwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIBAOBMKBYHrJ0zLhSbnKeXcmDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHguklz8vlYsuTV+aSJ6wkHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAiEA4FwIJhe8iRwZfMlmDgQCAcC4UAgHAiEA4FwIBAOBMKBQDgQCAcC4UAgHAhc8jyBMXYPRnJH27a55AkrCQcC4UAgHAiEA4FwIBAOBMKBQDgQCAcC4UDgIOHMQccFr+wLckImDgTCgUA4EAgHAuFAIBwIhAOBcCAQDgTCgUA4EAgHgvmS5zMdyvv1efsz39/XvwePY7J8a+JAIBwIhAOBcCAQDgTCgUA4EAgHAuFAIBwIhAOBcCC49tVp4B9MHAiEA4FwIBAOBMKBQDgQ/AUbRVOxdcl9dAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_trace(episode, end_state, end_reward) # states actions and rewards for each step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98I38ZuH18Lk",
        "outputId": "7d23fb56-b5bb-4697-fe61-5bf1a1eb0c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Time    State    Action  Reward\n",
            "0      1   (0, 4)    (1, 1)      -1\n",
            "1      2   (1, 5)    (1, 0)      -1\n",
            "2      3   (3, 6)    (0, 1)      -1\n",
            "3      4   (5, 8)   (0, -1)      -1\n",
            "4      5   (7, 9)   (0, -1)      -1\n",
            "5      6   (9, 9)  (-1, -1)      -1\n",
            "6      7  (10, 8)  (-1, -1)      -1\n",
            "7      8  (10, 6)    (1, 1)      -1\n",
            "8      9  (11, 5)    (1, 0)      -1\n",
            "9     10  (13, 4)    (1, 0)      -1\n",
            "10    11  (16, 3)         X      10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above three task, we can get an idea that if task is more complex/hard, then we should use larger rewards for earlier convergence. "
      ],
      "metadata": {
        "id": "Zxomk6nMoUSF"
      }
    }
  ]
}